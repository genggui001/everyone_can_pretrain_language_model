{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_pretrain.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAUc6LfMtIS4"
      },
      "source": [
        "# 代码参考 https://github.com/bojone/bert4keras/blob/master/pretraining/pretraining.py\r\n",
        "# 我只增加了加载优化器参数"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb41cHLZlTn5"
      },
      "source": [
        "# 基本环境配置"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwr75ihLkjAa",
        "outputId": "2e978a22-fe53-47f2-fd21-268716a6ff72"
      },
      "source": [
        "# 锁定版本\n",
        "\n",
        "! pip install tensorflow==2.4\n",
        "! pip install https://github.com/bojone/bert4keras/archive/3370e32862d419850d34f002ef3dd9e6b704e9e0.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/bojone/bert4keras/archive/3370e32862d419850d34f002ef3dd9e6b704e9e0.zip\n",
            "\u001b[?25l  Downloading https://github.com/bojone/bert4keras/archive/3370e32862d419850d34f002ef3dd9e6b704e9e0.zip\n",
            "\u001b[K     \\ 3.4MB 2.0MB/s\n",
            "\u001b[?25hCollecting keras<=2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 5.3MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras==0.9.7) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras==0.9.7) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras==0.9.7) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras==0.9.7) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras==0.9.7) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras==0.9.7) (3.13)\n",
            "Building wheels for collected packages: bert4keras\n",
            "  Building wheel for bert4keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert4keras: filename=bert4keras-0.9.7-cp36-none-any.whl size=47969 sha256=cf707e28d4362560c5c3bc142d2163104b82e114caf54ea90ea357d6967c4b31\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l_7lbr0t/wheels/9b/7a/e0/48d86836afa73cc40e5504f047855d9d9093cc6fc52f3a0868\n",
            "Successfully built bert4keras\n",
            "Installing collected packages: keras-applications, keras, bert4keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed bert4keras-0.9.7 keras-2.3.1 keras-applications-1.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFa9yW77kaRS",
        "outputId": "b220f0ab-be3c-4487-f554-368b7fdf7c5d"
      },
      "source": [
        "# 挂载谷歌网盘\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha4jXWnKnwsI"
      },
      "source": [
        "# 模型启动"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1oOXLodkyGj"
      },
      "source": [
        "import os, re\n",
        "os.environ['TF_KERAS'] = '1'  # 必须使用tf.keras\n",
        "\n",
        "import tensorflow as tf\n",
        "# 关闭eager模式！！！！极其重要\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# tf.random.set_seed(123456)\n",
        "# from pretraining.data_utils import *\n",
        "from bert4keras.models import build_transformer_model\n",
        "from bert4keras.backend import keras, K\n",
        "from bert4keras.optimizers import Adam\n",
        "from bert4keras.optimizers import extend_with_weight_decay\n",
        "from bert4keras.optimizers import extend_with_layer_adaptation\n",
        "from bert4keras.optimizers import extend_with_piecewise_linear_lr\n",
        "from bert4keras.optimizers import extend_with_gradient_accumulation\n",
        "from keras.layers import Input, Lambda\n",
        "from keras.models import Model\n",
        "from keras.callbacks import Callback, CSVLogger"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1c5attPkyJq"
      },
      "source": [
        "# 模型保存路径\n",
        "model_saved_path = '/content/gdrive/MyDrive/bert/model_saved_4096/'\n",
        "\n",
        "if not os.path.exists(model_saved_path):\n",
        "  os.makedirs(model_saved_path)\n",
        "\n",
        "# 语料路径 xxxxxxx 为对应的存储分区的路径\n",
        "corpus_paths = [\n",
        "    'gs://xxxxxxx/corpus_128.%s.tfrecord' % i for i in range(10)\n",
        "]\n",
        "\n",
        "# 其他配置\n",
        "sequence_length = 128\n",
        "batch_size = 4096\n",
        "\n",
        "learning_rate = 5 /  (pow(2, 1.5) * 1e3) # lamb 官方\n",
        "weight_decay_rate = 0.01\n",
        "num_warmup_steps = 12500\n",
        "num_train_steps = 125000\n",
        "steps_per_epoch = 10000\n",
        "grad_accum_steps = 8  # 大于1即表明使用梯度累积\n",
        "epochs = num_train_steps * grad_accum_steps // steps_per_epoch\n",
        "exclude_from_weight_decay = ['Norm', 'bias']\n",
        "which_optimizer = 'lamb'  # adam 或 lamb，均自带weight decay\n",
        "lr_schedule = {\n",
        "    num_warmup_steps * grad_accum_steps: 1.0,\n",
        "    num_train_steps * grad_accum_steps: 0.0,\n",
        "}\n",
        "floatx = K.floatx()\n",
        "\n",
        "# 是否加载优化器参数，默认True\n",
        "load_optimizer_weights = True"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nzH36bCWOh7"
      },
      "source": [
        "# bert_config\r\n",
        "\r\n",
        "config = {\r\n",
        "  \"attention_probs_dropout_prob\": 0.1, \r\n",
        "  \"directionality\": \"bidi\", \r\n",
        "  \"hidden_act\": \"gelu\", \r\n",
        "  \"hidden_dropout_prob\": 0.1, \r\n",
        "  \"hidden_size\": 768, \r\n",
        "  \"initializer_range\": 0.02, \r\n",
        "  \"intermediate_size\": 3072, \r\n",
        "  \"max_position_embeddings\": 512, \r\n",
        "  \"num_attention_heads\": 12, \r\n",
        "  \"num_hidden_layers\": 12, \r\n",
        "  \"pooler_fc_size\": 768, \r\n",
        "  \"pooler_num_attention_heads\": 12, \r\n",
        "  \"pooler_num_fc_layers\": 3, \r\n",
        "  \"pooler_size_per_head\": 128, \r\n",
        "  \"pooler_type\": \"first_token_transform\", \r\n",
        "  \"type_vocab_size\": 2, \r\n",
        "  \"vocab_size\": 21128\r\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j0QEcNlvtbK",
        "outputId": "f93f72c8-b62b-4522-bc38-e2c3f18ce4c9"
      },
      "source": [
        "# 加载最新参数与参数路径\r\n",
        "\r\n",
        "\r\n",
        "import re\r\n",
        "\r\n",
        "weights_files = os.listdir(model_saved_path)\r\n",
        "weights_files = [(int(re.findall(r\"epoch-(\\d+)-\", x)[0]), os.path.join(model_saved_path, x)) for x in weights_files]\r\n",
        "weights_files = sorted(weights_files, key=lambda x:x[1])\r\n",
        "\r\n",
        "initial_epoch = 0\r\n",
        "checkpoint_path = None\r\n",
        "\r\n",
        "if len(weights_files) != 0:\r\n",
        "  initial_epoch = weights_files[-1][0] + 1\r\n",
        "  checkpoint_path = weights_files[-1][1]\r\n",
        "\r\n",
        "print(initial_epoch)\r\n",
        "print(checkpoint_path)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eadIWlydu0Oj"
      },
      "source": [
        "class TrainingDataset(object):\r\n",
        "    @staticmethod\r\n",
        "    def load_tfrecord(record_names, batch_size, parse_function):\r\n",
        "        \"\"\"加载处理成tfrecord格式的语料\r\n",
        "        \"\"\"\r\n",
        "        if not isinstance(record_names, list):\r\n",
        "            record_names = [record_names]\r\n",
        "\r\n",
        "        dataset = tf.data.TFRecordDataset(record_names)  # 加载\r\n",
        "        dataset = dataset.map(parse_function)  # 解析\r\n",
        "        dataset = dataset.repeat()  # 循环\r\n",
        "        dataset = dataset.shuffle(batch_size * 1000)  # 打乱\r\n",
        "        dataset = dataset.batch(batch_size)  # 成批\r\n",
        "\r\n",
        "        return dataset\r\n",
        "\r\n",
        "class TrainingDatasetRoBERTa(TrainingDataset):\r\n",
        "    \"\"\"预训练数据集生成器（RoBERTa模式）\r\n",
        "    \"\"\"\r\n",
        "    @staticmethod\r\n",
        "    def load_tfrecord(record_names, sequence_length, batch_size):\r\n",
        "        \"\"\"给原方法补上parse_function\r\n",
        "        \"\"\"\r\n",
        "        def parse_function(serialized):\r\n",
        "            features = {\r\n",
        "                'token_ids': tf.io.FixedLenFeature([sequence_length], tf.int64),\r\n",
        "                'mask_ids': tf.io.FixedLenFeature([sequence_length], tf.int64),\r\n",
        "            }\r\n",
        "            features = tf.io.parse_single_example(serialized, features)\r\n",
        "            token_ids = features['token_ids']\r\n",
        "            mask_ids = features['mask_ids']\r\n",
        "            segment_ids = tf.zeros_like(token_ids, dtype='int64')\r\n",
        "            is_masked = tf.not_equal(mask_ids, 0)\r\n",
        "            masked_token_ids = K.switch(is_masked, mask_ids - 1, token_ids)\r\n",
        "            x = {\r\n",
        "                'Input-Token': masked_token_ids,\r\n",
        "                'Input-Segment': segment_ids,\r\n",
        "                'token_ids': token_ids,\r\n",
        "                'is_masked': tf.cast(is_masked, K.floatx()),\r\n",
        "            }\r\n",
        "            y = {\r\n",
        "                'mlm_loss': tf.zeros([1]),\r\n",
        "                'mlm_acc': tf.zeros([1]),\r\n",
        "            }\r\n",
        "            return x, y\r\n",
        "\r\n",
        "        return TrainingDataset.load_tfrecord(\r\n",
        "            record_names, batch_size, parse_function\r\n",
        "        )"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5QZD32kkyMt"
      },
      "source": [
        "dataset = TrainingDatasetRoBERTa.load_tfrecord(\n",
        "    record_names=corpus_paths,\n",
        "    sequence_length=sequence_length,\n",
        "    batch_size=batch_size // grad_accum_steps,\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "272V3Pl_pW6u"
      },
      "source": [
        "def build_transformer_model_with_mlm():\n",
        "    \"\"\"带mlm的bert模型\n",
        "    \"\"\"\n",
        "    bert = build_transformer_model(\n",
        "        model=\"bert\",\n",
        "        with_mlm='linear', return_keras_model=False, sequence_length=sequence_length,\n",
        "        **config\n",
        "    )\n",
        "\n",
        "    proba = bert.model.output\n",
        "\n",
        "    # 辅助输入\n",
        "    token_ids = Input(shape=(sequence_length,), dtype=tf.int64, name='token_ids')  # 目标id\n",
        "    is_masked = Input(shape=(sequence_length,), dtype=tf.float32, name='is_masked')  # mask标记\n",
        "\n",
        "    def mlm_loss(inputs):\n",
        "        \"\"\"计算loss的函数，需要封装为一个层\n",
        "        \"\"\"\n",
        "        y_true, y_pred, mask = inputs\n",
        "        loss = K.sparse_categorical_crossentropy(\n",
        "            y_true, y_pred, from_logits=True\n",
        "        )\n",
        "        loss = K.sum(loss * mask, axis=-1) / (K.sum(mask, axis=-1) + K.epsilon())\n",
        "        return loss\n",
        "\n",
        "    def mlm_acc(inputs):\n",
        "        \"\"\"计算准确率的函数，需要封装为一个层\n",
        "        \"\"\"\n",
        "        y_true, y_pred, mask = inputs\n",
        "        y_true = K.cast(y_true, tf.float32)\n",
        "        acc = keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "        acc = K.sum(acc * mask, axis=-1) / (K.sum(mask, axis=-1) + K.epsilon())\n",
        "        return acc\n",
        "\n",
        "    mlm_loss = Lambda(mlm_loss, name='mlm_loss')([token_ids, proba, is_masked])\n",
        "    mlm_acc = Lambda(mlm_acc, name='mlm_acc')([token_ids, proba, is_masked])\n",
        "\n",
        "    train_model = Model(\n",
        "        bert.model.inputs + [token_ids, is_masked], [mlm_loss, mlm_acc]\n",
        "    )\n",
        "\n",
        "    def mlm_loss(y_true, y_pred):\n",
        "      return y_pred\n",
        "\n",
        "    def mlm_acc(y_true, y_pred):\n",
        "      return K.stop_gradient(y_pred)\n",
        "\n",
        "    loss = {\n",
        "        'mlm_loss': mlm_loss,\n",
        "        'mlm_acc': mlm_acc,\n",
        "    }\n",
        "\n",
        "    return bert, train_model, loss"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdCAIjB81NF_"
      },
      "source": [
        "from bert4keras.optimizers import export_to_custom_objects\r\n",
        "\r\n",
        "# 梯度预归一化 参考 https://developer.nvidia.com/blog/pretraining-bert-with-layer-wise-adaptive-learning-rates/\r\n",
        "\r\n",
        "@export_to_custom_objects\r\n",
        "def extend_with_grad_norm(BaseOptimizer):\r\n",
        "    \"\"\"返回新的优化器类，加入梯度预归一化\r\n",
        "    \"\"\"\r\n",
        "    class NewOptimizer(BaseOptimizer):\r\n",
        "        def __init__(self, *args, **kwargs):\r\n",
        "            super(NewOptimizer, self).__init__(*args, **kwargs)\r\n",
        "\r\n",
        "        def _resource_apply(self, grad, var, indices=None):\r\n",
        "            op = super(NewOptimizer, self)._resource_apply(K.l2_normalize(grad), var, indices)\r\n",
        "            return op\r\n",
        "\r\n",
        "        def get_config(self):\r\n",
        "            config = {\r\n",
        "                'do_grad_norm':True,\r\n",
        "            }\r\n",
        "            base_config = super(NewOptimizer, self).get_config()\r\n",
        "            return dict(list(base_config.items()) + list(config.items()))\r\n",
        "\r\n",
        "    return NewOptimizer"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoJzBu-wpW9f"
      },
      "source": [
        "from tensorflow.python.keras.saving.hdf5_format import load_optimizer_weights_from_hdf5_group\n",
        "import h5py\n",
        "\n",
        "def build_transformer_model_for_pretraining():\n",
        "    \"\"\"构建训练模型，通用于TPU/GPU\n",
        "    注意全程要用keras标准的层写法，一些比较灵活的“移花接木”式的\n",
        "    写法可能会在TPU上训练失败。此外，要注意的是TPU并非支持所有\n",
        "    tensorflow算子，尤其不支持动态（变长）算子，因此编写相应运算\n",
        "    时要格外留意。\n",
        "    \"\"\"\n",
        "    bert, train_model, loss = build_transformer_model_with_mlm()\n",
        "\n",
        "\n",
        "    # 优化器\n",
        "    optimizer = extend_with_weight_decay(Adam)\n",
        "    # 梯度预归一化\n",
        "    # optimizer = extend_with_grad_norm(optimizer)\n",
        "    \n",
        "    if which_optimizer == 'lamb':\n",
        "        optimizer = extend_with_layer_adaptation(optimizer)\n",
        "    optimizer = extend_with_piecewise_linear_lr(optimizer)\n",
        "    optimizer_params = {\n",
        "        'learning_rate': learning_rate,\n",
        "        'lr_schedule': lr_schedule,\n",
        "        'weight_decay_rate': weight_decay_rate,\n",
        "        'exclude_from_weight_decay': exclude_from_weight_decay,\n",
        "        'exclude_from_layer_adaptation': exclude_from_weight_decay,\n",
        "        'bias_correction': True,\n",
        "    }\n",
        "    if grad_accum_steps > 1:\n",
        "        optimizer = extend_with_gradient_accumulation(optimizer)\n",
        "        optimizer_params['grad_accum_steps'] = grad_accum_steps\n",
        "\n",
        "    optimizer = optimizer(**optimizer_params)\n",
        "\n",
        "\n",
        "    # 模型定型\n",
        "    train_model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "    if checkpoint_path is not None:\n",
        "      train_model.load_weights(checkpoint_path)\n",
        "\n",
        "      if load_optimizer_weights:\n",
        "          with h5py.File(checkpoint_path, mode='r') as f:\n",
        "              train_model.optimizer._create_all_weights(train_model.trainable_variables)\n",
        "              optimizer_weight_values = load_optimizer_weights_from_hdf5_group(f)\n",
        "              \n",
        "              # 老参数修正梯度\n",
        "              if len(optimizer_weight_values) < len(train_model.optimizer.weights):\n",
        "                  # 迭代次数修正\n",
        "                  optimizer_weight_values[0] = np.array(0)\n",
        "                  # 增加梯度累计权重\n",
        "                  for var in train_model.trainable_variables:\n",
        "                      optimizer_weight_values.append(np.zeros(shape=var.shape))\n",
        "                  \n",
        "                  print(\"参数修复成功\")\n",
        "              \n",
        "              train_model.optimizer.set_weights(optimizer_weight_values)\n",
        "              print(\"优化器参数加载成功\")\n",
        "\n",
        "\n",
        "\n",
        "    return train_model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvmdCO8-pXAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b414785-b62e-4323-ee53-f0fb6afe3ea5"
      },
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "    train_model = build_transformer_model_for_pretraining()\n",
        "    train_model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on TPU  ['10.37.195.18:8470']\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.37.195.18:8470\n",
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.37.195.18:8470) for TPU system metadata.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.37.195.18:8470) for TPU system metadata.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, -8100468079493278489)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, -8100468079493278489)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 4621534235453659075)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 4621534235453659075)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -5044385415153988342)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -5044385415153988342)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 4075971976590159451)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 4075971976590159451)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -8398534712275970659)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -8398534712275970659)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -2060196543775666279)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -2060196543775666279)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -3742504486826295605)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -3742504486826295605)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5161740322800762161)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5161740322800762161)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -7586950388046568822)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -7586950388046568822)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, -5718637308463307987)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, -5718637308463307987)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7973639068636054144)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7973639068636054144)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (Embedding)     multiple             16226304    Input-Token[0][0]                \n",
            "                                                                 MLM-Norm[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 128, 768)     1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 128, 768)     0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 128, 768)     393216      Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 128, 768)     1536        Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 128, 768)     0           Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, 128, 768)     2362368     Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, 128, 768)     0           Embedding-Dropout[0][0]          \n",
            "                                                                 Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, 128, 768)     1536        Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward (Feed (None, 128, 768)     4722432     Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Dropo (None, 128, 768)     0           Transformer-0-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Add ( (None, 128, 768)     0           Transformer-0-MultiHeadSelfAttent\n",
            "                                                                 Transformer-0-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Norm  (None, 128, 768)     1536        Transformer-0-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, 128, 768)     2362368     Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-0-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, 128, 768)     1536        Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward (Feed (None, 128, 768)     4722432     Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Dropo (None, 128, 768)     0           Transformer-1-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Add ( (None, 128, 768)     0           Transformer-1-MultiHeadSelfAttent\n",
            "                                                                 Transformer-1-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Norm  (None, 128, 768)     1536        Transformer-1-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, 128, 768)     2362368     Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, 128, 768)     1536        Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward (Feed (None, 128, 768)     4722432     Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Dropo (None, 128, 768)     0           Transformer-2-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Add ( (None, 128, 768)     0           Transformer-2-MultiHeadSelfAttent\n",
            "                                                                 Transformer-2-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Norm  (None, 128, 768)     1536        Transformer-2-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, 128, 768)     2362368     Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, 128, 768)     1536        Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward (Feed (None, 128, 768)     4722432     Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Dropo (None, 128, 768)     0           Transformer-3-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Add ( (None, 128, 768)     0           Transformer-3-MultiHeadSelfAttent\n",
            "                                                                 Transformer-3-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Norm  (None, 128, 768)     1536        Transformer-3-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, 128, 768)     2362368     Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, 128, 768)     1536        Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward (Feed (None, 128, 768)     4722432     Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Dropo (None, 128, 768)     0           Transformer-4-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Add ( (None, 128, 768)     0           Transformer-4-MultiHeadSelfAttent\n",
            "                                                                 Transformer-4-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Norm  (None, 128, 768)     1536        Transformer-4-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, 128, 768)     2362368     Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, 128, 768)     1536        Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward (Feed (None, 128, 768)     4722432     Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Dropo (None, 128, 768)     0           Transformer-5-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Add ( (None, 128, 768)     0           Transformer-5-MultiHeadSelfAttent\n",
            "                                                                 Transformer-5-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Norm  (None, 128, 768)     1536        Transformer-5-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, 128, 768)     2362368     Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, 128, 768)     1536        Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward (Feed (None, 128, 768)     4722432     Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Dropo (None, 128, 768)     0           Transformer-6-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Add ( (None, 128, 768)     0           Transformer-6-MultiHeadSelfAttent\n",
            "                                                                 Transformer-6-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Norm  (None, 128, 768)     1536        Transformer-6-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, 128, 768)     2362368     Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, 128, 768)     1536        Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward (Feed (None, 128, 768)     4722432     Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Dropo (None, 128, 768)     0           Transformer-7-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Add ( (None, 128, 768)     0           Transformer-7-MultiHeadSelfAttent\n",
            "                                                                 Transformer-7-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Norm  (None, 128, 768)     1536        Transformer-7-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, 128, 768)     2362368     Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, 128, 768)     1536        Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward (Feed (None, 128, 768)     4722432     Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Dropo (None, 128, 768)     0           Transformer-8-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Add ( (None, 128, 768)     0           Transformer-8-MultiHeadSelfAttent\n",
            "                                                                 Transformer-8-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Norm  (None, 128, 768)     1536        Transformer-8-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, 128, 768)     2362368     Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, 128, 768)     0           Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, 128, 768)     1536        Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward (Feed (None, 128, 768)     4722432     Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Dropo (None, 128, 768)     0           Transformer-9-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Add ( (None, 128, 768)     0           Transformer-9-MultiHeadSelfAttent\n",
            "                                                                 Transformer-9-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Norm  (None, 128, 768)     1536        Transformer-9-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, 128, 768)     2362368     Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, 128, 768)     0           Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, 128, 768)     0           Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, 128, 768)     1536        Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward (Fee (None, 128, 768)     4722432     Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Drop (None, 128, 768)     0           Transformer-10-FeedForward[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Add  (None, 128, 768)     0           Transformer-10-MultiHeadSelfAtten\n",
            "                                                                 Transformer-10-FeedForward-Dropou\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Norm (None, 128, 768)     1536        Transformer-10-FeedForward-Add[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, 128, 768)     2362368     Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-10-FeedForward-Norm[0\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, 128, 768)     0           Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, 128, 768)     0           Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, 128, 768)     1536        Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward (Fee (None, 128, 768)     4722432     Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Drop (None, 128, 768)     0           Transformer-11-FeedForward[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Add  (None, 128, 768)     0           Transformer-11-MultiHeadSelfAtten\n",
            "                                                                 Transformer-11-FeedForward-Dropou\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Norm (None, 128, 768)     1536        Transformer-11-FeedForward-Add[0]\n",
            "__________________________________________________________________________________________________\n",
            "MLM-Dense (Dense)               (None, 128, 768)     590592      Transformer-11-FeedForward-Norm[0\n",
            "__________________________________________________________________________________________________\n",
            "MLM-Norm (LayerNormalization)   (None, 128, 768)     1536        MLM-Dense[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Bias (BiasAdd)              (None, 128, 21128)   21128       Embedding-Token[1][0]            \n",
            "__________________________________________________________________________________________________\n",
            "token_ids (InputLayer)          [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Activation (Activation)     (None, 128, 21128)   0           MLM-Bias[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "is_masked (InputLayer)          [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "mlm_loss (Lambda)               (None,)              0           token_ids[0][0]                  \n",
            "                                                                 MLM-Activation[0][0]             \n",
            "                                                                 is_masked[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "mlm_acc (Lambda)                (None,)              0           token_ids[0][0]                  \n",
            "                                                                 MLM-Activation[0][0]             \n",
            "                                                                 is_masked[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 102,290,312\n",
            "Trainable params: 102,290,312\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Rf9jz87pXFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee491e8-286f-4768-ab27-93fbb4984552"
      },
      "source": [
        "class ModelCheckpoint(keras.callbacks.Callback):\n",
        "    \"\"\"自动保存最新模型\n",
        "    \"\"\"\n",
        "    def __init__(self, model_saved_path):\n",
        "        self.model_saved_path = model_saved_path\n",
        "\n",
        "        if not os.path.exists(self.model_saved_path):\n",
        "          os.makedirs(self.model_saved_path)\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.model.save(\n",
        "          self.model_saved_path + \"epoch-%03d-mlm_loss-%.6f-mlm_acc-%.6f.h5\" % (epoch, logs['mlm_loss_loss'], logs['mlm_acc_loss']), \n",
        "          include_optimizer=True\n",
        "        )\n",
        "\n",
        "\n",
        "# 保存模型\n",
        "checkpoint = ModelCheckpoint(model_saved_path)\n",
        "\n",
        "# 模型训练\n",
        "train_model.fit(\n",
        "    dataset,\n",
        "    epochs=epochs,\n",
        "    initial_epoch=initial_epoch,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    callbacks=[checkpoint],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py:728: DistributedIteratorV1.initialize (from tensorflow.python.distribute.input_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use the iterator's `initializer` property instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py:728: DistributedIteratorV1.initialize (from tensorflow.python.distribute.input_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use the iterator's `initializer` property instead.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:492: UserWarning: `tf.keras.backend.learning_phase_scope` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.learning_phase_scope` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/100\n",
            "10000/10000 [==============================] - 8134s 675ms/step - batch: 4999.5000 - size: 1.0000 - num_steps: 1.0000 - loss: 3.2137 - mlm_loss_loss: 2.7654 - mlm_acc_loss: 0.4483\n",
            "Epoch 28/100\n",
            "10000/10000 [==============================] - 6548s 653ms/step - batch: 4999.5000 - size: 1.0000 - num_steps: 1.0000 - loss: 3.2766 - mlm_loss_loss: 2.8380 - mlm_acc_loss: 0.4386\n",
            "Epoch 29/100\n",
            "10000/10000 [==============================] - 6534s 652ms/step - batch: 4999.5000 - size: 1.0000 - num_steps: 1.0000 - loss: 3.2402 - mlm_loss_loss: 2.7905 - mlm_acc_loss: 0.4497\n",
            "Epoch 30/100\n",
            "10000/10000 [==============================] - 6497s 648ms/step - batch: 4999.5000 - size: 1.0000 - num_steps: 1.0000 - loss: 3.1739 - mlm_loss_loss: 2.7155 - mlm_acc_loss: 0.4585\n",
            "Epoch 31/100\n",
            "10000/10000 [==============================] - 6457s 644ms/step - batch: 4999.5000 - size: 1.0000 - num_steps: 1.0000 - loss: 3.2160 - mlm_loss_loss: 2.7679 - mlm_acc_loss: 0.4480\n",
            "Epoch 32/100\n",
            "10000/10000 [==============================] - 6460s 644ms/step - batch: 4999.5000 - size: 1.0000 - num_steps: 1.0000 - loss: 3.1908 - mlm_loss_loss: 2.7337 - mlm_acc_loss: 0.4571\n",
            "Epoch 33/100\n",
            "10000/10000 [==============================] - 6462s 644ms/step - batch: 4999.5000 - size: 1.0000 - num_steps: 1.0000 - loss: 3.1310 - mlm_loss_loss: 2.6654 - mlm_acc_loss: 0.4656\n",
            "Epoch 34/100\n",
            "10000/10000 [==============================] - 6463s 644ms/step - batch: 4999.5000 - size: 1.0000 - num_steps: 1.0000 - loss: 3.1679 - mlm_loss_loss: 2.7122 - mlm_acc_loss: 0.4556\n",
            "Epoch 35/100\n",
            "10000/10000 [==============================] - 6462s 644ms/step - batch: 4999.5000 - size: 1.0000 - num_steps: 1.0000 - loss: 3.1539 - mlm_loss_loss: 2.6913 - mlm_acc_loss: 0.4626\n",
            "Epoch 36/100\n",
            "10000/10000 [==============================] - 6464s 644ms/step - batch: 4999.5000 - size: 1.0000 - num_steps: 1.0000 - loss: 3.0947 - mlm_loss_loss: 2.6229 - mlm_acc_loss: 0.4718\n",
            "Epoch 37/100\n",
            "10000/10000 [==============================] - 6463s 645ms/step - batch: 4999.5000 - size: 1.0000 - num_steps: 1.0000 - loss: 3.1272 - mlm_loss_loss: 2.6652 - mlm_acc_loss: 0.4620\n",
            "Epoch 38/100\n",
            "10000/10000 [==============================] - 6464s 645ms/step - batch: 4999.5000 - size: 1.0000 - num_steps: 1.0000 - loss: 3.1222 - mlm_loss_loss: 2.6548 - mlm_acc_loss: 0.4674\n",
            "Epoch 39/100\n",
            "10000/10000 [==============================] - 6462s 644ms/step - batch: 4999.5000 - size: 1.0000 - num_steps: 1.0000 - loss: 3.0645 - mlm_loss_loss: 2.5876 - mlm_acc_loss: 0.4769\n",
            "Epoch 40/100\n",
            "  444/10000 [>.............................] - ETA: 1:42:27 - batch: 221.5000 - size: 1.0000 - num_steps: 1.0000 - loss: 3.2745 - mlm_loss_loss: 2.8304 - mlm_acc_loss: 0.4441"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}